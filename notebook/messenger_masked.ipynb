{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c4ce498-3ba0-45a3-b6bd-726032fee1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed9f46fd-8d8c-44e1-9723-5594196fe705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {\n",
    "    'conversation_start': '[CSTART]',\n",
    "    'conversation_end': '[CEND]',\n",
    "    'message_start': '[MSTART]',\n",
    "    'message_end': '[MEND]',\n",
    "    'writes': '[WRITES]',\n",
    "}\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": list(special_tokens.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7dc6df-7043-43d0-a64a-2d0d69cb6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"../data/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c794701-316b-471e-a664-b648c57a3393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50257, 50259,  8071,    89, 50261,    13,    67,  3919,   133,   117,\n",
       "           130,   225,   220,   133,   247,   133,    98,   134,   229,   279,\n",
       "           133,   247,   134,   229,   133,   238,   133,   247,   133,   117,\n",
       "           133,   242,   645,   134,   236, 50260, 50259,  8071,    89, 50261,\n",
       "            65,   321, 50260, 50259, 40656, 10903,   263, 50261, 13681,    11,\n",
       "           881,  4577,   284,   900,   510, 27220,  1826,  4739, 50260, 50259,\n",
       "          8071,    89, 50261,  2959,   929, 50260, 50259, 24778, 22586, 39423,\n",
       "         50261,   403, 16841,  1826,  4739,    30,   367,    76,   986,   635,\n",
       "          1900,   355, 10938,   503, 50260, 50259, 24778, 22586, 39423, 50261,\n",
       "          3919,   761,   284,   307,  8668,  1058,    79, 50260, 50259, 24778,\n",
       "         22586, 39423, 50261,  2396,   356,   547,  3612,   286, 16853,  2229,\n",
       "           319,  3502,    30, 50260, 50259,  8071,    89, 50261, 40798,   407,\n",
       "            11,  1312,   716,  1016,   284,  3187,   257,  1545]),\n",
       " tensor([ -100, 50259,  8071,    89, 50261,    13,    67,  3919,   133,   117,\n",
       "           130,   225,   220,   133,   247,   133,    98,   134,   229,   279,\n",
       "           133,   247,   134,   229,   133,   238,   133,   247,   133,   117,\n",
       "           133,   242,   645,   134,   236,  -100, 50259,  8071,    89, 50261,\n",
       "            65,   321,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 50259,\n",
       "          8071,    89, 50261,  2959,   929,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100, 50259,  8071,    89, 50261, 40798,   407,\n",
       "            11,  1312,   716,  1016,   284,  3187,   257,  1545]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MessengerDataset(Dataset):\n",
    "    @classmethod\n",
    "    def generate_mask(cls, text, tokenizer, whitelist):\n",
    "        mstart = tokenizer(\"[MSTART]\").input_ids[0]\n",
    "        writes = tokenizer(\"[WRITES]\").input_ids[0]\n",
    "        mend = tokenizer(\"[MEND]\").input_ids[0]\n",
    "        # wh = [tokenizer(nick).input_ids for nick in whitelist]\n",
    "        mask = np.zeros(len(text))\n",
    "        mstarts = np.where(text == mstart)[0]\n",
    "        writess = np.where(text == writes)[0]\n",
    "        mends = np.where(text == mend)[0]\n",
    "        for ms, wr, me in zip(mstarts, writess, mends):\n",
    "            nick = tokenizer.decode(text[ms+1 : wr])\n",
    "            if nick in whitelist:\n",
    "                mask[ms:me] = 1  \n",
    "        return mask\n",
    "        \n",
    "    def __init__(self, text_path, tokenizer, block_size, whitelist):\n",
    "        assert os.path.isfile(text_path), f\"Input file path {text_path} not found\"\n",
    "        self.examples = []\n",
    "        with open(text_path, encoding='utf-8') as f:\n",
    "            text = f.read()[:100000]\n",
    "        tokenized_text = np.array(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))\n",
    "        mask = MessengerDataset.generate_mask(tokenized_text, tokenizer, whitelist)\n",
    "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "            self.examples.append((tokenized_text[i:i+block_size], mask[i:i+block_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs, mask = self.examples[idx]\n",
    "        labels = inputs.copy()\n",
    "        labels[mask == 0] = -100\n",
    "        return torch.as_tensor(inputs), torch.as_tensor(labels)\n",
    "    \n",
    "ds = MessengerDataset(text_file, tokenizer, 128, [\"patz\"])\n",
    "ds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
