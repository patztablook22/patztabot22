{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099e8583-53b4-4039-b3f3-ababbf5fc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9b75133b-49a9-448b-a44d-a7841e003c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_bpe = Tokenizer.from_file('../data/bpe_bytelevel.json')\n",
    "tok_bbpe = Tokenizer.from_file('../data/bbpe.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca39bcd4-d8c1-4439-ba67-05658dcbe7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb9a209a-6256-4293-9f05-f6a1b020936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 51 bpe 50 bbpe 46\n",
      "[4919, 881, 4898, 561, 257, 4898, 354, 1347, 20539, 611, 257, 4898, 354, 1347, 714, 20539, 4898, 11, 1312, 4240, 986, 262, 4898, 354, 1347, 561, 20539, 355, 881, 4898, 355, 285, 81, 13, 2330, 714, 611, 257, 4898, 354, 794, 4255, 20539, 18396, 27406, 285, 81, 13, 2330, 1453, 68]\n",
      "[329, 475, 2335, 227, 70, 2335, 218, 656, 1588, 252, 70, 2335, 218, 656, 216, 1588, 2335, 11, 84, 977, 12825, 13, 80, 2335, 218, 656, 227, 1588, 150, 475, 2335, 150, 1081, 13, 1309, 216, 252, 70, 2335, 218, 11839, 2601, 1588, 8864, 21058, 1081, 13, 1309, 1594, 41]\n",
      "[997, 665, 2527, 416, 258, 2527, 16914, 1803, 464, 258, 2527, 16914, 405, 1803, 2527, 11, 279, 1182, 9128, 267, 2527, 16914, 416, 1803, 341, 665, 2527, 341, 1416, 13, 1507, 405, 464, 258, 2527, 382, 8210, 2789, 1803, 9682, 22926, 1416, 13, 1507, 1746, 68]\n",
      "\n",
      "how much wood would a woodchuck chuck if a woodchuck could chuck wood, i wonder... the woodchuck would chuck as much wood as mr. white could if a woodchuch cook chuck dude yo mr. whiteeee\n",
      "how much wood would a woodchuck chuck if a woodchuck could chuck wood, i wonder... the woodchuck would chuck as much wood as mr. white could if a woodchuch cook chuck dude yo mr. whiteeee\n",
      "how much wood would a woodchuck chuck if a woodchuck could chuck wood, i wonder... the woodchuck would chuck as much wood as mr. white could if a woodchuch cook chuck dude yo mr. whiteeee\n",
      "\n",
      " for but announ�g announ\u001e",
      " into large�g announ\u001e",
      " into\u001c",
      " large announ,uoun1000.q announ\u001e",
      " into� large� but announ� As. let\u001c",
      "�g announ\u001e",
      "アCl large................................ Melee As. let 199J\n",
      "Ġrosie ars Ġmagic ase Ġcl Ġmagic Ġare Ġvictoria hammer Ġph Ġcl Ġmagic Ġare Ġvictoria Ġgave hammer Ġmagic , osed bbed Ġtre Ġjust Ġmagic Ġare Ġvictoria ase hammer ine ars Ġmagic ine very Ġ' . fast Ġgave Ġph Ġcl Ġmagic Ġare oy Ġwit hammer Ġchooses Ġkabul very Ġ' . fast Ġcoffee he\n",
      "field next shiftick t shiftshection moistened bec t shiftshection ye moistened shift, prom idiotaybe w shiftshectionick moistened had next shift had lr. kissing ye bec t shiftshe talibb moistened dab cahill lr. kissingingse\n"
     ]
    }
   ],
   "source": [
    "text = \"how much wood would a woodchuck chuck if a woodchuck could chuck wood, i wonder... the woodchuck would chuck as much wood as mr. white could if a woodchuch cook chuck dude yo mr. whiteeee\"\n",
    "output_gpt2 = tok_gpt2(text).input_ids\n",
    "output_bpe = tok_bpe.encode(text).ids\n",
    "output_bbpe = tok_bbpe.encode(text).ids\n",
    "\n",
    "def bpe_decode(ids):\n",
    "    ts = [tok_bpe.id_to_token(i) for i in ids]\n",
    "    for i in range(len(ts)):\n",
    "        if ts[i][0] == 'Ġ':\n",
    "            ts[i] = ('' if i == 0 else ' ') + ts[i][1:]\n",
    "    return ''.join(ts)\n",
    "\n",
    "print(f'gpt2 {len(output_gpt2)} bpe {len(output_bpe)} bbpe {len(output_bbpe)}')\n",
    "print(output_gpt2)\n",
    "print(output_bpe)\n",
    "print(output_bbpe)\n",
    "print()\n",
    "\n",
    "print(tok_gpt2.decode(output_gpt2))\n",
    "print(bpe_decode(output_bpe))\n",
    "print(tok_bbpe.decode(output_bbpe))\n",
    "print()\n",
    "\n",
    "print(tok_gpt2.decode(output_bpe))\n",
    "print(tok_bpe.decode(output_gpt2))\n",
    "print(tok_bbpe.decode(output_gpt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
