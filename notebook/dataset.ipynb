{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc596781-0000-45a8-beff-2d4a70a4f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, re\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "sys.path.insert(0, '../src/')\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9804c1b-2b92-4624-b9e5-97522fe84628",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'conversation_start': '[CSTART]',\n",
    "    'conversation_end': '[CEND]',\n",
    "    'message_start': '[MSTART]',\n",
    "    'breakpoint': '[BREAK]',\n",
    "    'message_end': '[MEND]',\n",
    "    'writes': '[WRITES]',\n",
    "    'avoid': '[AVOID]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade84c7d-d7c8-4278-8381-b2734d335122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "s = dataset.load_simulated_conversations('../data/simulated_conversations')\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d43c15-f9d1-4081-8fa1-7723a517b2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "m = dataset.load_messenger_conversations('/home/patz/Heap/messages/')\n",
    "m = [dataset.rename_conversation_user(c, 'Patztablook TwentyTwo', 'you') for c in m]\n",
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efdce96-4ac7-4ec2-a7d0-aa8aac9447a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "m1 = [dataset.mark_avoid(c, url_pattern, special_tokens) for c in m]\n",
    "s1 = [dataset.mark_avoid(c, url_pattern, special_tokens) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8842e7c-1271-4fc5-9b24-8435afb40b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "def message_filter(content):\n",
    "    if content is None: return True # empty messages will be used later\n",
    "\n",
    "    # application message, spam\n",
    "    content = content.strip()\n",
    "    blacklist = ['ə', 'ɥ', 'ʇ', 'ʎ', 'ɹ', 'ƃ',\n",
    "                'se svymi geny? 0=ne', 're you satisfied with ur genes?', 'ould you describe urself as',\n",
    "                'nyom', \"hen you want to retain something in your\", \"thy weight and height\"]\n",
    "    for b in blacklist:\n",
    "        if b in content: return False\n",
    "    \n",
    "    # wall of text\n",
    "    if len(content) > 128 or len(content.splitlines()) > 4: return False\n",
    "    \n",
    "    # reference to a previous message\n",
    "    if content in ['.', '*'] : return False\n",
    "\n",
    "    # fixing a previous message (probably a typo):\n",
    "    if content[0] == '*' and content[1] != ' ' and len(content.split(' ')) <= 2 and content.count('*') == 1: return False\n",
    "    if content[-1] == '*' and content[-2] != ' ' and len(content.split(' ')) <= 2 and content.count('*') == 1: return False\n",
    "\n",
    "    # decrease the frequency of overused phrase messages\n",
    "    rates = {\"ik\": 0.7,\n",
    "             \"irl\": 0.5,\n",
    "             \"xd\": 0.7}\n",
    "    words = [w for w in re.split(r'(\\W)', content) if w.strip()]\n",
    "    for r in rates:\n",
    "        if len(words) <= 2 and b in words:\n",
    "            if np.random.random() < 1 - rates[r]:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "m2 = [dataset.filter_messages_by_content(c, message_filter) for c in m1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c829e880-34a4-4581-a9a5-26f9a5cabec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = list(filter(lambda c: len(c) >= 64, m2))\n",
    "len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503b6a1f-1209-4f04-bd2c-1d7d595741d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 2694)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['alice', 'bob', 'martin', 'john', 'alex', 'mary', 'filip', 'martha', 'elisa', 'peter', 'paul', 'emil', 'honza', 'cutecat1', 'WIZARD', 'E', '0x01',\n",
    "         'george', 'jirka', 'aneta', 'owo', 'daniel', 'jaroslav', 'xi', 'william', 'bill', 'harrison']\n",
    "s2 = [c1 for c in s1 for c1 in dataset.sample_names(c, 'User1', names, 3)]\n",
    "\n",
    "m2 = [c1 for c in m2 for c1 in dataset.split(c, message_limit=100)]\n",
    "\n",
    "len(s2), len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17faa143-d15a-45df-a9da-7e39c5abdcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694\n"
     ]
    }
   ],
   "source": [
    "val_indices = [] #[-1, -3, -13]\n",
    "train_indices = np.delete(np.arange(len(m2)), val_indices)\n",
    "\n",
    "print(len(m2))\n",
    "for i in val_indices:\n",
    "    print(m2[i].participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20618d0d-fe92-4dcf-a805-108c252d2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "train_conversations = [m2[i] for i in train_indices] + s2\n",
    "train_conversations = np.random.permutation(np.array(train_conversations, dtype=object))\n",
    "\n",
    "val_conversations = [m2[i] for i in val_indices]\n",
    "\n",
    "dataset.generate_corpus('../data/train.txt', train_conversations, special_tokens, break_tollerance_s=180)\n",
    "dataset.generate_corpus('../data/val.txt', val_conversations, special_tokens, break_tollerance_s=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870224c-5a4b-4675-b905-a456535cb1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
